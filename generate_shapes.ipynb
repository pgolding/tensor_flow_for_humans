{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Some Shapes\n",
    "\n",
    "This will generate a bunch of circles and squares and prepare the data for streaming to Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_circles(size=28,number_of_shapes=100):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _ in tqdm(range(number_of_shapes),desc='Circle', unit='circles'):\n",
    "        blank_image = Image.new('L', (size, size), 'white')\n",
    "        img_draw = ImageDraw.Draw(blank_image)\n",
    "        x0 = random.randint(0,int(size/5))\n",
    "        x1 = random.randint(int(2*size/3),size)\n",
    "        img_draw.arc([x0,x0,x1,x1], start=0, end=360, fill='blue')\n",
    "        blank_image = blank_image.resize((28,28))\n",
    "        #blank_image = blank_image.rotate(random.randint(-10,10))\n",
    "        feature = np.array(blank_image, dtype=np.float32).flatten()\n",
    "        label = 'circle'\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_squares(size=28,number_of_shapes=100):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _ in tqdm(range(number_of_shapes),desc='Square', unit='squares'):\n",
    "        blank_image = Image.new('L', (size, size), 'white')\n",
    "        img_draw = ImageDraw.Draw(blank_image)\n",
    "        x0 = random.randint(0,int(size/5))\n",
    "        x1 = random.randint(int(size/3),size)\n",
    "        img_draw.rectangle([x0,x0,x1,x1],  outline='blue')\n",
    "        blank_image = blank_image.resize((28,28))\n",
    "        feature = np.array(blank_image, dtype=np.float32).flatten()\n",
    "        label = 'square'\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Circle: 100%|██████████| 50000/50000 [00:04<00:00, 11039.10circles/s]\n",
      "Square: 100%|██████████| 50000/50000 [00:03<00:00, 14006.89squares/s]\n",
      "Circle: 100%|██████████| 1000/1000 [00:00<00:00, 11048.75circles/s]\n",
      "Square: 100%|██████████| 1000/1000 [00:00<00:00, 14361.20squares/s]\n"
     ]
    }
   ],
   "source": [
    "#make some training data\n",
    "circle_features, circle_labels = make_circles(size=28,number_of_shapes=50000)\n",
    "square_features, square_labels = make_squares(size=28,number_of_shapes=50000)\n",
    "train_features = np.array(circle_features + square_features)\n",
    "train_labels = np.array(circle_labels + square_labels)\n",
    "\n",
    "#make some test data\n",
    "circle_features, circle_labels = make_circles(size=28,number_of_shapes=1000)\n",
    "square_features, square_labels = make_squares(size=28,number_of_shapes=1000)\n",
    "test_features = np.array(circle_features + square_features)\n",
    "test_labels = np.array(circle_labels + square_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Data - \"Normalization\"\n",
    "\n",
    "Here we scale the data to have the same \"dynamic range\" - see [this link](http://cs231n.github.io/neural-networks-2/#datapre) or [this video](https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs)\n",
    "\n",
    "Note that our features are already similarly scaled because any pixel in the image can take a similar range of values (given their synthetic creation method above). However, we still (linearly) scale here to map our data to the same numeric range as our expected activation function (e.g. relu and softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pixel_levels(features):\n",
    "    lower_val = 0.0\n",
    "    upper_val = 1.0\n",
    "    pixel_min = 0\n",
    "    pixel_max = 255\n",
    "    return lower_val + (features - pixel_min)*(upper_val-lower_val)/(pixel_max-pixel_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = normalize_pixel_levels(train_features)\n",
    "test_features = normalize_pixel_levels(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_train = label_encoder.fit_transform(train_labels)\n",
    "integer_encoded_test = label_encoder.fit_transform(test_labels)\n",
    "\n",
    "\n",
    "#binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "integer_encoded_train = integer_encoded_train.reshape(len(integer_encoded_train), 1)\n",
    "train_labels = onehot_encoder.fit_transform(integer_encoded_train)\n",
    "\n",
    "integer_encoded_test = integer_encoded_test.reshape(len(integer_encoded_test), 1)\n",
    "test_labels = onehot_encoder.fit_transform(integer_encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'shape_data.pickle'\n",
    "\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as pf:\n",
    "        pickle.dump(\n",
    "        {\n",
    "            'train_dataset': train_features,\n",
    "            'train_labels': train_labels,\n",
    "            'valid_dataset': valid_features,\n",
    "            'valid_labels': valid_labels,\n",
    "            'test_dataset': test_features,\n",
    "            'test_labels': test_labels,\n",
    "        },\n",
    "        pf, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
